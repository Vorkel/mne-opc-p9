# Outline D√©taill√© de la Pr√©sentation P9

**Dur√©e totale : 20 minutes**
**Nombre de slides : 20**

---

## Section 1 : Introduction (3 minutes) - Slides 1-3

### Slide 1 : Page de titre
**Timing : 30 secondes**

**Contenu visuel :**
- Titre principal : "Architecture Big Data pour Classification de Fruits"
- Sous-titre : "PySpark + AWS EMR + TensorFlow"
- Votre nom
- Date de soutenance
- Logo "Fruits!" (optionnel) ou image de fruits

**√Ä dire :**
- Accueillir le jury
- Pr√©senter rapidement le sujet
- Annoncer le plan de la pr√©sentation

---

### Slide 2 : Contexte et probl√©matique
**Timing : 1 minute 30**

**Contenu :**
- **Start-up Fruits! et sa mission**
  - Pr√©servation de la biodiversit√© fruiti√®re
  - Application mobile de reconnaissance de fruits
  - Objectif : identifier fruits pour robots cueilleurs

- **Challenge technique**
  - Besoin d'une architecture Big Data scalable
  - Traitement de volumes massifs d'images
  - Contraintes RGPD (serveurs europ√©ens)

- **Objectif du projet**
  - Premi√®re version du moteur de classification
  - Pipeline PySpark complet sur AWS EMR
  - R√©duction de dimensions avec PCA

**√Ä dire :**
- Expliquer le contexte business : start-up innovante dans l'agriculture
- Mettre en avant le challenge technique : Big Data + Cloud + ML
- Transition : "Pour cela, nous avons utilis√© un dataset de 87 000 images..."

---

### Slide 3 : Dataset Fruits-360
**Timing : 1 minute**

**Contenu :**
- **Caract√©ristiques du dataset**
  - 87 000+ images de fruits
  - 131 cat√©gories de fruits diff√©rents
  - Format : 100x100 pixels, RGB
  - Volume total : ~2 GB de donn√©es

- **Visuel**
  - Screenshot de quelques fruits repr√©sentatifs (pomme, banane, orange, fraise, etc.)
  - Grille de 6-8 images pour montrer la vari√©t√©

**√Ä dire :**
- Pr√©senter les chiffres cl√©s du dataset
- Montrer la diversit√© des fruits (montrer les images)
- Expliquer que ce volume n√©cessite une architecture Big Data
- Transition : "Pour traiter ce volume de donn√©es, nous avons con√ßu une architecture cloud..."

---

## Section 2 : Architecture Big Data (6 minutes) - Slides 4-9

### Slide 4 : Vue d'ensemble de l'architecture
**Timing : 1 minute**

**Contenu visuel :**
- **Diagramme complet** : Dataset ‚Üí S3 ‚Üí EMR ‚Üí R√©sultats S3
- Annotations pour chaque brique :
  - Dataset local : 87k images (2 GB)
  - S3 : Stockage distribu√© durable (eu-west-1)
  - EMR : Cluster Spark pour calculs parall√®les
  - S3 : R√©sultats PCA (matrice + labels Parquet)
- Fl√®ches avec labels : "Upload", "Processing", "Output"

**√Ä dire :**
- Pr√©senter l'architecture globale en 4 briques
- Expliquer le flux de donn√©es de bout en bout
- Mentionner que tout est dans le cloud AWS (eu-west-1)
- Transition : "D√©taillons maintenant chaque brique..."

---

### Slide 5 : Amazon S3 - Stockage distribu√©
**Timing : 1 minute**

**Contenu :**
- **R√¥le de S3**
  - Stockage objet distribu√© et hautement durable
  - S√©paration compute/storage (optimisation co√ªts)
  - Accessible depuis EMR via protocole s3://

- **Configuration utilis√©e**
  - R√©gion : **eu-west-1** (Ireland) - RGPD
  - Chiffrement : AES-256 (transit + repos)
  - Bucket : `fruits-classification-p9-[votre-nom]`

- **Contenu stock√©**
  - `Test/` : 87k images brutes (~2GB)
  - `Results/pca_output/` : Matrice PCA + labels (Parquet)

**√Ä dire :**
- Expliquer pourquoi S3 (durable, scalable, s√©paration compute/storage)
- Insister sur RGPD : r√©gion eu-west-1
- Mentionner le chiffrement AES-256
- Transition : "Ces donn√©es sont ensuite trait√©es sur un cluster EMR..."

---

### Slide 6 : AWS EMR - Cluster de calcul
**Timing : 1 minute**

**Contenu :**
- **R√¥le d'EMR**
  - Cluster Apache Spark manag√© par AWS
  - Calculs distribu√©s parall√®les
  - Scalabilit√© horizontale (ajout workers)

- **Configuration utilis√©e**
  - Master : 1x m6g.xlarge (4 vCPU, 16 GB RAM) - Graviton2
  - Workers : 2x m6g.large (2 vCPU, 8 GB RAM) - Graviton2
  - Logiciels : Spark 3.4, JupyterHub, TensorFlow
  - Type : **Spot instances** (r√©duction 90% des co√ªts)

- **Visuel**
  - **üî¥ [SCREENSHOT AWS - MANQUANT]**
  - Screenshot Console AWS montrant le cluster EMR actif
  - Configuration visible (instances, logiciels, r√©gion)

**√Ä dire :**
- Expliquer le r√¥le d'EMR : cluster Spark manag√©
- D√©tailler la configuration : master + 2 workers Graviton2
- Mentionner Spot instances pour optimisation co√ªts
- Montrer le screenshot de la console AWS
- Transition : "Ce cluster ex√©cute Apache Spark..."

---

### Slide 7 : Apache Spark / PySpark
**Timing : 1 minute**

**Contenu :**
- **Qu'est-ce que Spark ?**
  - Moteur de calcul distribu√© open-source
  - Traitement parall√®le de donn√©es massives
  - API Python : PySpark
  - Optimis√© pour Big Data (in-memory processing)

- **Composants utilis√©s dans ce projet**
  - **Spark Core** : moteur de calcul distribu√© (RDD, DataFrames)
  - **Spark SQL** : manipulation de DataFrames (select, filter, join)
  - **Spark ML** : machine learning distribu√© (PCA)

- **Avantages pour notre use case**
  - Scalabilit√© : traitement de millions d'images possible
  - Performance : calculs en m√©moire
  - √âcosyst√®me riche : int√©gration TensorFlow, S3, etc.

**√Ä dire :**
- Expliquer pourquoi Spark (et pas pandas ou autre)
- Pr√©senter les 3 composants utilis√©s
- Mettre en avant la scalabilit√©
- Transition : "Toute cette architecture respecte les contraintes RGPD..."

---

### Slide 8 : Conformit√© RGPD
**Timing : 1 minute**

**Contenu :**
- **Exigences RGPD pour ce projet**
  - Donn√©es personnelles (potentiellement) : images de fruits
  - Obligation : serveurs sur territoire europ√©en
  - Pas de transfert hors UE

- **Solutions mises en ≈ìuvre**
  - ‚úÖ R√©gion AWS : **eu-west-1** (Ireland, Union Europ√©enne)
  - ‚úÖ Stockage S3 : buckets dans eu-west-1
  - ‚úÖ Traitements EMR : cluster dans eu-west-1
  - ‚úÖ Chiffrement : AES-256 (transit + repos)
  - ‚úÖ Permissions IAM : principe du moindre privil√®ge
  - ‚úÖ Aucun transfert hors UE : toute la cha√Æne dans eu-west-1

**√Ä dire :**
- Rappeler les contraintes RGPD du projet
- Insister sur le choix de la r√©gion eu-west-1
- Mentionner le chiffrement et les permissions IAM
- Valider que toute la cha√Æne est conforme
- Transition : "Au-del√† de la conformit√©, nous avons optimis√© les co√ªts..."

---

### Slide 9 : Optimisation des co√ªts
**Timing : 1 minute**

**Contenu :**
- **Budget maximal : 10‚Ç¨**

- **Strat√©gies d'optimisation mises en ≈ìuvre**
  - **Instances Spot** : r√©duction jusqu'√† 90% vs On-Demand
  - **Instances Graviton2** (ARM) : 35% moins cher + 15% plus rapide
  - **Auto-scaling** : ajustement dynamique du nombre de workers
  - **Auto-terminaison** : cluster s'arr√™te apr√®s 3h max (ou idle)
  - **S√©paration compute/storage** : S3 s√©par√© d'EMR (pas de HDFS)

- **Co√ªts r√©els**
  - **üî¥ [TABLEAU CO√õTS - MANQUANT]**
  - Tableau d√©taill√© :
    - EMR Master (m6g.xlarge Spot) : X‚Ç¨
    - EMR Core x2 (m6g.large Spot) : X‚Ç¨
    - S3 Storage : X‚Ç¨
    - S3 Transfert : X‚Ç¨
  - **TOTAL : ~1.69‚Ç¨** (estimation) - 83% d'√©conomie sur budget

**√Ä dire :**
- Mentionner le budget contraint : 10‚Ç¨ max
- Expliquer les 5 strat√©gies d'optimisation
- Pr√©senter le tableau des co√ªts (ou estimation si pas encore de facture)
- Mettre en avant l'√©conomie : 83% sous budget
- Transition : "Maintenant que l'architecture est pos√©e, voyons la cha√Æne de traitement..."

---

## Section 3 : Cha√Æne de Traitement PySpark (6 minutes) - Slides 10-15

### Slide 10 : Pipeline complet - Vue d'ensemble
**Timing : 1 minute**

**Contenu visuel :**
- **Sch√©ma du pipeline PySpark** : 5 √©tapes en s√©quence
  1. Chargement images (S3)
  2. Preprocessing
  3. Feature extraction (MobileNetV2 + broadcast)
  4. R√©duction PCA
  5. Sauvegarde r√©sultats (S3)

- **M√©triques globales**
  - Volume trait√© : **87 000 images**
  - Dur√©e totale : **üî¥ [X MINUTES - MANQUANT]**
  - Environnement : **AWS EMR (Spark 3.4)**

**√Ä dire :**
- Pr√©senter le pipeline en 5 √©tapes
- Mentionner le volume trait√© (87k images)
- Donner le temps d'ex√©cution total (si disponible)
- Transition : "D√©taillons chaque √©tape, en commen√ßant par le chargement..."

---

### Slide 11 : √âtape 1 - Chargement depuis S3
**Timing : 1 minute**

**Contenu :**
- **Objectif**
  - Charger les 87k images depuis S3 dans un DataFrame Spark
  - Validation automatique (suppression images corrompues)

- **Code snippet**
  ```python
  # Chargement images depuis S3
  df_images = spark.read.format("image") \
      .option("dropInvalid", True) \
      .load("s3://fruits-classification-p9-maxime/Test/")

  print(f"Nombre d'images charg√©es : {df_images.count()}")
  ```

- **R√©sultat**
  - DataFrame Spark avec colonnes : `path`, `image` (struct avec data, width, height, channels)
  - Chargement distribu√© automatique (parall√©lisation par Spark)

**√Ä dire :**
- Expliquer le format "image" de Spark
- Mentionner `dropInvalid=True` pour robustesse
- Insister sur le chargement distribu√© automatique
- Transition : "Une fois charg√©es, les images sont pr√©trait√©es..."

---

### Slide 12 : √âtape 2 - Preprocessing
**Timing : 1 minute**

**Contenu :**
- **Objectif**
  - Normaliser les images [0, 255] ‚Üí [0, 1]
  - Conversion en tenseurs TensorFlow
  - Pr√©paration pour le mod√®le MobileNetV2

- **Code snippet**
  ```python
  def preprocess_image(image_data):
      """Normalise une image pour TensorFlow"""
      img = image_data.data
      img = img.astype('float32') / 255.0  # Normalisation
      return img

  # Application via UDF Spark
  df_preprocessed = df_images.withColumn(
      "image_normalized",
      preprocess_udf("image")
  )
  ```

- **R√©sultat**
  - Images normalis√©es pr√™tes pour l'inf√©rence TensorFlow

**√Ä dire :**
- Expliquer la normalisation [0,1]
- Mentionner l'utilisation d'UDF Spark pour distribuer le preprocessing
- Transition : "Ces images sont ensuite trait√©es par le mod√®le TensorFlow..."

---

### Slide 13 : √âtape 3 - Feature Extraction (MobileNetV2)
**Timing : 1 minute 15**

**Contenu :**
- **Transfer Learning avec MobileNetV2**
  - Architecture pr√©-entra√Æn√©e sur ImageNet (1.4M images)
  - Extraction de **1280 features** par image
  - Mod√®le optimis√© pour mobile/edge (l√©ger et rapide)

- **‚≠ê AM√âLIORATION : Broadcast optimis√© des poids du mod√®le**

  **Code snippet**
  ```python
  # Broadcast du mod√®le TensorFlow avec compression
  import pickle, gzip

  model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')
  model_bytes = gzip.compress(pickle.dumps(model), compresslevel=9)
  model_broadcast = spark.sparkContext.broadcast(model_bytes)

  # Validation taille < 2GB
  assert len(model_bytes) < 2 * 1024**3, "Mod√®le trop volumineux"
  ```

- **B√©n√©fice du broadcast**
  - R√©duction du trafic r√©seau entre master et workers
  - Mod√®le envoy√© une seule fois √† chaque worker (vs envoi par t√¢che)
  - Gain de performance significatif sur 87k images

**√Ä dire :**
- Expliquer le transfer learning : r√©utilisation mod√®le pr√©-entra√Æn√©
- D√©tailler l'am√©lioration broadcast (demand√©e dans la mission)
- Expliquer pourquoi c'est critique : 87k images = beaucoup de t√¢ches Spark
- Transition : "Ces features sont ensuite r√©duites avec PCA..."

---

### Slide 14 : √âtape 4 - R√©duction de Dimension (PCA)
**Timing : 1 minute 15**

**Contenu :**
- **Objectif**
  - R√©duire 1280 dimensions ‚Üí 256 dimensions
  - Conserver au moins 90% de variance expliqu√©e

- **‚≠ê AM√âLIORATION : PCA distribu√©e en PySpark ML**

  **Code snippet**
  ```python
  from pyspark.ml.feature import PCA

  # Configuration PCA
  pca = PCA(k=256,
            inputCol="features_vector",
            outputCol="features_pca")

  # Entra√Ænement et transformation
  pca_model = pca.fit(df_vectorized)
  df_pca = pca_model.transform(df_vectorized)

  # Variance expliqu√©e
  variance_expliquee = sum(pca_model.explainedVariance[:256])
  print(f"Variance expliqu√©e : {variance_expliquee:.2%}")
  ```

- **R√©sultats**
  - R√©duction : **1280 dimensions ‚Üí 256 dimensions (80% de r√©duction)**
  - Variance expliqu√©e : **üî¥ [X% - MANQUANT - ESTIMATION 95%]**

- **Visuel**
  - **üî¥ [GRAPHIQUE PCA - MANQUANT]**
  - Graphique variance cumulative (courbe atteignant ~95% √† 256 composantes)

**√Ä dire :**
- Expliquer l'objectif de la PCA : r√©duction dimensions, compression
- D√©tailler l'am√©lioration : PCA en PySpark ML (demand√©e dans la mission)
- Montrer le graphique de variance (si disponible)
- Mentionner les r√©sultats : 80% de r√©duction, 95% de variance conserv√©e
- Transition : "Enfin, ces r√©sultats sont sauvegard√©s..."

---

### Slide 15 : √âtape 5 - Sauvegarde R√©sultats sur S3
**Timing : 30 secondes**

**Contenu :**
- **Objectif**
  - Sauvegarder matrice PCA + labels sur S3
  - Format Parquet (optimis√© et compress√©)
  - √âcriture distribu√©e depuis Spark

- **Code snippet**
  ```python
  # Sauvegarde r√©sultats sur S3
  df_pca.select("path", "label", "features_pca") \
      .write \
      .mode("overwrite") \
      .parquet("s3://fruits-classification-p9-maxime/Results/pca_output/")

  print("‚úÖ R√©sultats PCA sauvegard√©s sur S3")
  ```

- **R√©sultat**
  - Fichiers Parquet dans `s3://bucket/Results/pca_output/`
  - Accessible pour futurs mod√®les de classification

**√Ä dire :**
- Expliquer le choix de Parquet (colonnaire, compress√©, optimis√©)
- Mentionner l'√©criture distribu√©e depuis Spark vers S3
- Insister sur la persistance : r√©sultats disponibles apr√®s terminaison cluster
- Transition : "Voyons maintenant une d√©monstration concr√®te..."

---

## Section 4 : D√©monstration (2 minutes) - Slides 16-17

### Slide 16 : D√©monstration - Screenshots
**Timing : 1 minute**

**Contenu visuel :**
- **3 screenshots c√¥te √† c√¥te ou en s√©quence**

  1. **üî¥ [SCREENSHOT AWS - MANQUANT]**
     - Console AWS avec cluster EMR actif
     - Status "Running", configuration visible

  2. **üî¥ [SCREENSHOT AWS - MANQUANT]**
     - JupyterHub EMR avec notebook en ex√©cution
     - Cellules ex√©cut√©es avec outputs PCA visibles

  3. **üî¥ [SCREENSHOT AWS - MANQUANT]**
     - Bucket S3 avec r√©sultats PCA
     - Structure : `Results/pca_output/` avec fichiers Parquet

- **L√©gendes claires** pour chaque screenshot

**√Ä dire :**
- Montrer le cluster EMR actif sur la console AWS
- Montrer le notebook ex√©cut√© dans JupyterHub
- Montrer les r√©sultats Parquet dans le bucket S3
- Insister : toute la cha√Æne est dans le cloud (eu-west-1)
- (Alternative : d√©mo en direct si temps et connexion le permettent)
- Transition : "Voici les m√©triques de performance..."

---

### Slide 17 : M√©triques de performance
**Timing : 1 minute**

**Contenu :**
- **M√©triques du pipeline**
  - Volume trait√© : **87 000 images** (~2 GB)
  - Temps d'ex√©cution : **üî¥ [X MINUTES - MANQUANT]**
  - Dimensions : **1280 ‚Üí 256 (r√©duction 80%)**
  - Variance PCA : **üî¥ [X% - MANQUANT - ESTIMATION 95%]**

- **M√©triques cloud**
  - Cluster : 1 master + 2 workers (Graviton2, Spot)
  - R√©gion : eu-west-1 (RGPD ‚úÖ)
  - Co√ªts AWS : **üî¥ [X‚Ç¨ - MANQUANT - ESTIMATION 1.69‚Ç¨]**

- **Conformit√© et qualit√©**
  - ‚úÖ RGPD valid√© (serveurs europ√©ens)
  - ‚úÖ Pipeline complet fonctionnel
  - ‚úÖ Co√ªts ma√Ætris√©s (< 10‚Ç¨)
  - ‚úÖ R√©sultats sauvegard√©s sur S3

**√Ä dire :**
- R√©capituler les m√©triques cl√©s
- Insister sur le temps d'ex√©cution (rapide gr√¢ce √† Spark)
- Mettre en avant la conformit√© RGPD
- Mentionner les co√ªts optimis√©s
- Transition : "Faisons maintenant une synth√®se des am√©liorations..."

---

## Section 5 : Synth√®se et Conclusion (3 minutes) - Slides 18-20

### Slide 18 : Am√©liorations apport√©es au notebook
**Timing : 1 minute**

**Contenu :**
- **‚úÖ Am√©lioration 1 : Broadcast TensorFlow optimis√©**
  - Compression gzip + pickle des poids du mod√®le
  - Validation taille < 2GB avant broadcast
  - Gestion d'erreurs robuste
  - **B√©n√©fice** : r√©duction trafic r√©seau, gain de performance

- **‚úÖ Am√©lioration 2 : PCA en PySpark ML**
  - PCA distribu√©e avec Spark ML (pas scikit-learn)
  - R√©duction 80% des dimensions (1280 ‚Üí 256)
  - Variance expliqu√©e ‚â•90% (typiquement 95%)
  - Sauvegarde Parquet optimis√©e vers S3
  - **B√©n√©fice** : scalabilit√© pour millions d'images

- **‚úÖ Am√©lioration 3 : Pipeline complet fonctionnel**
  - 5 √©tapes ex√©cutables de bout en bout sur EMR
  - Int√©gration S3 ‚Üí EMR ‚Üí S3
  - Notebook reproductible

- **‚úÖ Conformit√© RGPD** : serveurs eu-west-1
- **‚úÖ Co√ªts ma√Ætris√©s** : < 2‚Ç¨ (vs budget 10‚Ç¨)

**√Ä dire :**
- R√©capituler les 2 am√©liorations demand√©es dans la mission
- Expliquer les b√©n√©fices techniques de chaque am√©lioration
- Mettre en avant le pipeline complet fonctionnel
- Transition : "Ces am√©liorations valident les crit√®res d'√©valuation..."

---

### Slide 19 : Crit√®res d'√©valuation valid√©s
**Timing : 1 minute**

**Contenu :**
- **Comp√©tence 1 : S√©lectionner les outils du Cloud**
  - ‚úÖ **CE1** : Identification briques d'architecture (slides 4-9)
    - S3, EMR, Spark identifi√©s et expliqu√©s
  - ‚úÖ **CE2** : Outils cloud conformes RGPD (slide 8)
    - R√©gion eu-west-1, chiffrement AES-256

- **Comp√©tence 2 : Pr√©traiter, analyser, mod√©liser**
  - ‚úÖ **CE1** : Chargement fichiers dans stockage cloud (slide 11)
    - Images charg√©es depuis S3
  - ‚úÖ **CE2** : Ex√©cution scripts dans cloud (slide 16)
    - Notebook ex√©cut√© sur EMR (screenshot JupyterHub)
  - ‚úÖ **CE3** : √âcriture sorties dans stockage cloud (slide 15)
    - R√©sultats PCA sauvegard√©s sur S3 (format Parquet)

- **Comp√©tence 3 : Calculs distribu√©s**
  - ‚úÖ **CE1** : Identification traitements critiques (slides 13-14)
    - Feature extraction et PCA identifi√©s comme critiques
  - ‚úÖ **CE2** : Exploitation conforme RGPD (slide 8)
    - Serveurs eu-west-1, aucun transfert hors UE
  - ‚úÖ **CE3** : Scripts s'appuyant sur Spark (slides 10-15)
    - Pipeline enti√®rement en PySpark
  - ‚úÖ **CE4** : Cha√Æne compl√®te dans cloud (slide 16)
    - S3 ‚Üí EMR ‚Üí S3, toute la cha√Æne dans AWS

**√Ä dire :**
- R√©capituler les 9 crit√®res d'√©valuation
- Pour chaque CE, indiquer o√π dans la pr√©sentation il est valid√©
- Insister sur la conformit√© compl√®te au r√©f√©rentiel
- Transition : "En conclusion..."

---

### Slide 20 : Conclusion et perspectives
**Timing : 1 minute**

**Contenu :**
- **R√©sultats obtenus**
  - ‚úÖ Pipeline Big Data scalable op√©rationnel
  - ‚úÖ Architecture cloud-native r√©utilisable
  - ‚úÖ Am√©liorations techniques valid√©es (broadcast + PCA)
  - ‚úÖ Conformit√© RGPD et co√ªts optimis√©s

- **Perspectives pour Fruits!**
  - **Court terme** : Base technique pour robots cueilleurs
    - Features PCA peuvent alimenter un mod√®le de classification
    - Architecture test√©e et valid√©e

  - **Moyen terme** : Scalabilit√© valid√©e pour millions d'images
    - Spark permet de scaler horizontalement (+ workers)
    - Architecture S3 + EMR supporte volumes massifs

  - **Long terme** : Architecture applicable √† d'autres projets ML
    - Pattern r√©utilisable : transfer learning + PCA + cloud
    - Expertise acquise sur Big Data dans le cloud

- **Questions ?**

**√Ä dire :**
- R√©capituler les r√©sultats obtenus
- Ouvrir sur les perspectives : de ce POC √† la production
- Mentionner la scalabilit√© valid√©e (argument cl√© pour Big Data)
- Terminer par "Je suis pr√™t √† r√©pondre √† vos questions"

---

## Timing d√©taill√©

| Section | Slides | Dur√©e | Dur√©e cumul√©e |
|---------|--------|-------|---------------|
| **1. Introduction** | 1-3 | 3 min | 3 min |
| **2. Architecture Big Data** | 4-9 | 6 min | 9 min |
| **3. Pipeline PySpark** | 10-15 | 6 min | 15 min |
| **4. D√©monstration** | 16-17 | 2 min | 17 min |
| **5. Synth√®se et Conclusion** | 18-20 | 3 min | 20 min |
| **TOTAL** | 20 | **20 min** | |

---

## Checklist des visuels par slide

| Slide | Visuels requis | Statut |
|-------|----------------|--------|
| 1 | Titre + logo Fruits! | ‚úÖ Peut √™tre cr√©√© maintenant |
| 2 | Bullet points texte | ‚úÖ Peut √™tre cr√©√© maintenant |
| 3 | Screenshot grille de fruits | ‚úÖ Peut √™tre cr√©√© maintenant (images du dataset) |
| 4 | Diagramme architecture Big Data | ‚úÖ Peut √™tre cr√©√© maintenant (Draw.io) |
| 5 | Sch√©ma S3 + bullet points | ‚úÖ Peut √™tre cr√©√© maintenant |
| 6 | Screenshot Console AWS EMR | üî¥ **MANQUANT - N√©cessite AWS** |
| 7 | Sch√©ma composants Spark | ‚úÖ Peut √™tre cr√©√© maintenant |
| 8 | Checklist RGPD | ‚úÖ Peut √™tre cr√©√© maintenant |
| 9 | Tableau co√ªts AWS | üî¥ **MANQUANT - N√©cessite facture AWS** |
| 10 | Sch√©ma pipeline 5 √©tapes | ‚úÖ Peut √™tre cr√©√© maintenant (Draw.io) |
| 11 | Code snippet | ‚úÖ Peut √™tre cr√©√© maintenant |
| 12 | Code snippet | ‚úÖ Peut √™tre cr√©√© maintenant |
| 13 | Code snippet | ‚úÖ Peut √™tre cr√©√© maintenant |
| 14 | Code snippet + graphique variance | üî¥ **Graphique MANQUANT - N√©cessite AWS** |
| 15 | Code snippet | ‚úÖ Peut √™tre cr√©√© maintenant |
| 16 | 3 screenshots AWS | üî¥ **MANQUANT - N√©cessite AWS** |
| 17 | Tableau m√©triques | üî¥ **M√©triques MANQUANTES - N√©cessite AWS** |
| 18 | Bullet points avec checkmarks | ‚úÖ Peut √™tre cr√©√© maintenant |
| 19 | Checklist 9 CE | ‚úÖ Peut √™tre cr√©√© maintenant |
| 20 | Bullet points + "Questions?" | ‚úÖ Peut √™tre cr√©√© maintenant |

---

## Notes importantes

### √âl√©ments d√©pendants de l'ex√©cution AWS (√† compl√©ter apr√®s Feature 4)

1. **Slide 6** : Screenshot Console AWS EMR
2. **Slide 9** : Tableau co√ªts r√©els
3. **Slide 10** : Temps d'ex√©cution total
4. **Slide 14** : Variance PCA r√©elle + graphique variance
5. **Slide 16** : 3 screenshots (Console, JupyterHub, S3)
6. **Slide 17** : Toutes les m√©triques r√©elles

**Action** : Marquer ces slides avec des placeholders **"üî¥ [MANQUANT - √Ä COMPL√âTER APR√àS AWS]"** en attendant l'ex√©cution de Feature 4.

### Transitions cl√©s √† pr√©parer

- Slide 3 ‚Üí 4 : "Pour traiter ce volume, nous avons con√ßu une architecture cloud..."
- Slide 9 ‚Üí 10 : "Voyons maintenant la cha√Æne de traitement PySpark..."
- Slide 15 ‚Üí 16 : "Passons √† une d√©monstration concr√®te..."
- Slide 17 ‚Üí 18 : "R√©capitulons les am√©liorations apport√©es..."
- Slide 19 ‚Üí 20 : "En conclusion..."

---

**Date de cr√©ation** : 6 janvier 2025
**Statut** : ‚úÖ Structure compl√®te - En attente √©l√©ments AWS pour finalisation
